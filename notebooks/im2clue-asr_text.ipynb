{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb0c959-b9fb-496b-aeb2-7394720f2f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%autosave 60\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e35c9f5-f56b-4a6f-b7af-3f52493fde30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pickle\n",
    "from collections import Counter, OrderedDict\n",
    "from copy import deepcopy\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Optional, Tuple, Union, cast\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import PIL.Image as pil_img\n",
    "import seaborn as sns\n",
    "import sklearn as skl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from IPython.display import Image, display\n",
    "from matplotlib.patches import Rectangle\n",
    "from matplotlib_inline.backend_inline import set_matplotlib_formats\n",
    "from torch.nn import functional as F\n",
    "from tqdm.contrib import tenumerate, tmap, tzip\n",
    "from tqdm.contrib.bells import tqdm, trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8103a868-7e93-49f2-a31a-256bec5cea4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "pd.set_option(\"display.max_columns\", 15)\n",
    "pd.set_option(\"display.max_rows\", 50)\n",
    "# Suitable default display for floats\n",
    "pd.options.display.float_format = \"{:,.2f}\".format\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 10)\n",
    "\n",
    "# This one is optional -- change graphs to SVG only use if you don't have a\n",
    "# lot of points/lines in your graphs. Can also just use ['retina'] if you\n",
    "# don't want SVG.\n",
    "%config InlineBackend.figure_formats = [\"retina\"]\n",
    "set_matplotlib_formats(\"pdf\", \"png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c5600f-15d7-4e80-b5ec-62326f0be086",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = json.load(open(\"/shared/g-luo/geoguessr/data/data/guidebook/kb/v3/cleaned_clues.json\"))\n",
    "clues = data[\"clues\"]\n",
    "clue_types = data[\"clue_types\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f308e74c-7696-4e46-b81c-f0d7200445c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sims(a: torch.tensor, b: torch.tensor, batch_size=1000):\n",
    "    sims = {i: (0, -1) for i in range(a.shape[0])}\n",
    "    for j in tqdm(range(0, b.shape[0], batch_size)):\n",
    "        batch_sims = a @ b[j : j + batch_size].T\n",
    "        batch_sims = torch.from_numpy(batch_sims)\n",
    "        values, idxs = batch_sims.max(dim=-1)\n",
    "        # Append an index and sim every iteration\n",
    "        for i in range(a.shape[0]):\n",
    "            offset = j + idxs[i].item()\n",
    "            if values[i] > sims[i][0]:\n",
    "                sims[i] = (values[i].item(), offset)\n",
    "    return sims\n",
    "\n",
    "\n",
    "def normalize_rows(mat: torch.tensor) -> None:\n",
    "    for i in range(len(mat)):\n",
    "        mat[i] /= mat[i].norm(p=2, dim=-1, keepdim=True)\n",
    "    return mat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5334148b-d47a-4613-a93a-562a2ba60f2b",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "\n",
    "## Geoguessr In-game Frames -> ASR Text Lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a398cd-9ca2-4fca-b3bf-d2ee1e7c923c",
   "metadata": {},
   "outputs": [],
   "source": [
    "split = \"test\"\n",
    "# Load query image embeddings:\n",
    "image_embs = pickle.load(\n",
    "    open(\n",
    "        f\"/shared/gbiamby/geo/models/clip_ft/vit-b32/geoframes_clip_samples_fixed_window_{split}_img.pkl\",\n",
    "        \"rb\",\n",
    "    )\n",
    ")\n",
    "text_embs = pickle.load(\n",
    "    open(\n",
    "        f\"/shared/gbiamby/geo/models/clip_ft/vit-b32/geoframes_clip_samples_fixed_window_{split}_text.pkl\",\n",
    "        \"rb\",\n",
    "    )\n",
    ")\n",
    "# load target captions:\n",
    "with open(\n",
    "    f\"/shared/gbiamby/geo/captions/clip_samples_fixed_window_{split}.json\", encoding=\"utf-8\"\n",
    ") as f:\n",
    "    sent_bert_caps = json.load(f)\n",
    "\n",
    "# Append embeddings\n",
    "for caption in sent_bert_caps:\n",
    "    caption[\"clip_emb\"] = image_embs[caption[\"file_path\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be42f167-f234-45b6-9c2f-b43babb393b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_image_embs = normalize_rows(\n",
    "    torch.stack([torch.tensor(emb) for emb in image_embs.values()]).to(\"cuda\")\n",
    ")\n",
    "t_text_embs = normalize_rows(\n",
    "    torch.stack([torch.tensor(emb) for emb in text_embs.values()]).to(\"cuda\")\n",
    ")\n",
    "\n",
    "print(f\"image_embs.shape: {t_image_embs.shape}, text_emb.shape: {t_text_embs.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616ca091-8387-43f3-8b78-df79723dc065",
   "metadata": {},
   "outputs": [],
   "source": [
    "sims = torch.mm(t_image_embs, t_text_embs.T)\n",
    "max_sim_scores, max_sim_idxs = sims.max(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4dbd596-308e-4e7d-acac-df35b221bb3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_unique = list(text_embs.keys())\n",
    "img_to_text_sims = {}\n",
    "for i, img_path in enumerate(image_embs.keys()):\n",
    "    img_to_text_sims[img_path] = {\n",
    "        \"best_match_text\": texts_unique[max_sim_idxs[i]],\n",
    "        \"best_sim_score\": max_sim_scores[i].tolist(),\n",
    "        \"best_sim_idx\": max_sim_idxs[i].tolist(),\n",
    "        \"sims_all\": sims[i].tolist(),\n",
    "        \"file_path\": img_path,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0edc9d02-949d-4b52-8c37-2f98a01a1a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(img_to_text_sims.items())[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc84496-829f-400b-aa00-cc60b04c44e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import HTML, Markdown\n",
    "\n",
    "\n",
    "def show_samples(img_to_text_sims, n_samples: int = 20):\n",
    "    df_random = np.random.choice(range(len(img_to_text_sims)), n_samples, replace=False)\n",
    "\n",
    "    for i in df_random:\n",
    "        img_row = deepcopy(img_to_text_sims[i])\n",
    "        del img_row[\"sims_all\"]\n",
    "        print(\"=\" * 180)\n",
    "        # print(img_row.keys())\n",
    "        # print(img_row)\n",
    "        display(pd.DataFrame({k: [v] for k, v in img_row.items()}).T)\n",
    "        img = pil_img.open(img_row[\"file_path\"])\n",
    "        img.thumbnail((1080, 640), pil_img.NEAREST)\n",
    "        display(img)\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e26b56-a9c6-40da-9f2f-71767c866297",
   "metadata": {},
   "source": [
    "### Choose Images w/ Highest Img/Text Similarity Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53e1f0a-96e9-49a9-8ee0-e78c05bfdec9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "show_samples(\n",
    "    sorted(img_to_text_sims.values(), key=lambda x: x[\"best_sim_score\"], reverse=True)[:100],\n",
    "    n_samples=20,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f5870b-b81e-4d1c-ad42-54d5a7189c98",
   "metadata": {},
   "source": [
    "### Choose Random Images, show best match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7707f492-11c4-4bc3-80b4-6d6b0f6726c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "show_samples(list(img_to_text_sims.values()), 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0597e40c-00ee-4cc3-9703-3aa37caf5695",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446ac09e-b926-4a3d-8e4e-0a54fde5e694",
   "metadata": {},
   "source": [
    "## Junk"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geoscreens",
   "language": "python",
   "name": "geoscreens"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
