{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pickle\n",
    "import sys\n",
    "\n",
    "import clip\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "from geoscreens.geo_utils import gcd_threshold_eval, vectorized_gc_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"ViT-B/32\"\n",
    "pretrained_path = \"/shared/gbiamby/im2gps_kb/lib/open_clip/logs/lr=1e-06_wd=0.1_agg=True_model=ViT-B32_batchsize=96_workers=4_date=2022-03-15-04-33-55/epoch_2.pt\"\n",
    "device = \"cuda\"\n",
    "model, preprocess = clip.load(model_name, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if pretrained_path:\n",
    "    state_dict = torch.load(pretrained_path)[\"state_dict\"]\n",
    "    for key in list(state_dict.keys()):\n",
    "        state_dict[key.replace(\"module.\", \"\")] = state_dict.pop(key)\n",
    "    model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_image(images):\n",
    "    images = images.to(device)\n",
    "    with torch.no_grad():\n",
    "        image_embedddings = model.encode_image(images)\n",
    "        image_embedddings /= image_embedddings.norm(dim=-1, keepdim=True)\n",
    "    return image_embedddings\n",
    "\n",
    "\n",
    "def get_image_embeddings(image_paths, batch_size=100):\n",
    "    image_embeddings = []\n",
    "    images = []\n",
    "    for i in tqdm(range(0, len(image_paths), batch_size)):\n",
    "        batch_images = [Image.open(image_path) for image_path in image_paths[i : i + batch_size]]\n",
    "        images_tensor = torch.vstack([preprocess(image).unsqueeze(0) for image in batch_images])\n",
    "        image_embeddings.append(embed_image(images_tensor))\n",
    "        images.extend(batch_images)\n",
    "    image_embeddings = torch.vstack(image_embeddings)\n",
    "    return images, image_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CLIP Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference = json.load(open(\"/shared/g-luo/geoguessr/data/placing2014/placing2014_no_indoor.json\"))\n",
    "streetview = pd.read_csv(\"/shared/g-luo/geoguessr/data/streetview/val/val.csv\")\n",
    "streetview = streetview.to_dict(orient=\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get flickr embeddings\n",
    "flickr_embeddings = pickle.load(\n",
    "    open(\n",
    "        \"/shared/gbiamby/geo/models/clip_ft_contrastive_00/vit-b32/placing2014_reference.pkl\", \"rb\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# Get streetview embeddings\n",
    "folder = \"/shared/g-luo/geoguessr/data/streetview/val/cutter\"\n",
    "streetview_image_paths = [f\"{folder}/{s['IMG_ID']}\" for s in streetview]\n",
    "streetview_images, streetview_embeddings = get_image_embeddings(streetview_image_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_ids = list(flickr_embeddings.keys())\n",
    "s_ids = [s[\"IMG_ID\"] for s in streetview]\n",
    "flickr_embeddings = torch.vstack([torch.from_numpy(r) for r in flickr_embeddings.values()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference = {r[\"hash\"]: r for r in reference}\n",
    "streetview = {s[\"IMG_ID\"]: s for s in streetview}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "streetview_embeddings, flickr_embeddings = streetview_embeddings.to(device), flickr_embeddings.to(\n",
    "    device\n",
    ")\n",
    "sims = {}\n",
    "for s in tqdm(range(streetview_embeddings.shape[0])):\n",
    "    embed_sims = streetview_embeddings[s] @ flickr_embeddings.T\n",
    "    values, idxs = torch.topk(embed_sims, dim=-1, k=1)\n",
    "    s_id = s_ids[s]\n",
    "    r_id = r_ids[idxs.item()]\n",
    "    sims[s_id] = (values.item(), r_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each index, get the ground truth GPS\n",
    "latitudes, longitudes = [], []\n",
    "latitudes_gt, longitudes_gt = [], []\n",
    "for s_id, (prob, r_id) in sims.items():\n",
    "    s = streetview[s_id]\n",
    "    r = reference[r_id]\n",
    "\n",
    "    latitudes.append(r[\"LAT\"])\n",
    "    longitudes.append(r[\"LON\"])\n",
    "\n",
    "    latitudes_gt.append(s[\"LAT\"])\n",
    "    longitudes_gt.append(s[\"LON\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latitudes, longitudes, latitudes_gt, longitudes_gt = (\n",
    "    torch.Tensor(latitudes),\n",
    "    torch.Tensor(longitudes),\n",
    "    torch.Tensor(latitudes_gt),\n",
    "    torch.Tensor(longitudes_gt),\n",
    ")\n",
    "distances = vectorized_gc_distance(latitudes, longitudes, latitudes_gt, longitudes_gt)\n",
    "gcd_threshold_eval(distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geoscreens",
   "language": "python",
   "name": "geoscreens"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
