{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pickle\n",
    "import sys\n",
    "\n",
    "import clip\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "from geoscreens.geo_utils import gcd_threshold_eval, vectorized_gc_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"ViT-B/32\"\n",
    "# pretrained_path = \"/shared/gbiamby/im2gps_kb/lib/open_clip/logs/lr=1e-06_wd=0.1_agg=True_model=ViT-B32_batchsize=96_workers=4_date=2022-03-15-04-33-55/epoch_2.pt\"\n",
    "# model_name = \"RN50x16\"\n",
    "# pretrained_path = \"/shared/gbiamby/geo/models/clip/ft_with_binary_classifier/best.ckpt\"\n",
    "device = \"cuda\"\n",
    "model, preprocess = clip.load(model_name, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pretrained_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mpretrained_path\u001b[49m:\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.ckpt\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m pretrained_path:\n\u001b[1;32m      3\u001b[0m         state_dict \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(pretrained_path)[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pretrained_path' is not defined"
     ]
    }
   ],
   "source": [
    "if pretrained_path:\n",
    "    if \".ckpt\" in pretrained_path:\n",
    "        state_dict = torch.load(pretrained_path)[\"model\"]\n",
    "    else:\n",
    "        state_dict = torch.load(pretrained_path)[\"state_dict\"]\n",
    "    for key in list(state_dict.keys()):\n",
    "        state_dict[key.replace(\"module.\", \"\")] = state_dict.pop(key)\n",
    "        state_dict[key.replace(\"model.\", \"\")] = state_dict.pop(key)\n",
    "    model.load_state_dict(state_dict, strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_image(images):\n",
    "    images = images.to(device)\n",
    "    with torch.no_grad():\n",
    "        image_embedddings = model.encode_image(images)\n",
    "        image_embedddings /= image_embedddings.norm(dim=-1, keepdim=True)\n",
    "    return image_embedddings\n",
    "\n",
    "\n",
    "def get_image_embeddings(image_paths, batch_size=100):\n",
    "    image_embeddings = []\n",
    "    images = []\n",
    "    for i in tqdm(range(0, len(image_paths), batch_size)):\n",
    "        batch_images = [Image.open(image_path) for image_path in image_paths[i : i + batch_size]]\n",
    "        images_tensor = torch.vstack([preprocess(image).unsqueeze(0) for image in batch_images])\n",
    "        image_embeddings.append(embed_image(images_tensor))\n",
    "        images.extend(batch_images)\n",
    "    image_embeddings = torch.vstack(image_embeddings)\n",
    "    return images, image_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CLIP Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference = json.load(open(\"/shared/g-luo/geoguessr/data/placing2014/placing2014_no_indoor.json\"))\n",
    "streetview = pd.read_csv(\"/shared/g-luo/geoguessr/data/streetview/val/val.csv\")\n",
    "streetview = streetview.to_dict(orient=\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [01:19<00:00,  7.96s/it]\n"
     ]
    }
   ],
   "source": [
    "# Get flickr embeddings\n",
    "flickr_embeddings = pickle.load(\n",
    "    open(\n",
    "        # \"/shared/gbiamby/geo/models/clip_ft_contrastive_00/vit-b32/placing2014_reference.pkl\", \"rb\"\n",
    "        \"/shared/gbiamby/geo/models/clip_ft/vit-b32_zeroshot/placing2014_reference_image.pkl\", \"rb\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# Get streetview embeddings\n",
    "folder = \"/shared/g-luo/geoguessr/data/streetview/val/cutter\"\n",
    "streetview_image_paths = [f\"{folder}/{s['IMG_ID']}\" for s in streetview]\n",
    "streetview_images, streetview_embeddings = get_image_embeddings(streetview_image_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_ids = list(flickr_embeddings.keys())\n",
    "s_ids = [s[\"IMG_ID\"] for s in streetview]\n",
    "flickr_embeddings = torch.vstack([torch.from_numpy(r) for r in flickr_embeddings.values()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference = {r[\"hash\"]: r for r in reference}\n",
    "streetview = {s[\"IMG_ID\"]: s for s in streetview}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:05<00:00, 198.28it/s]\n"
     ]
    }
   ],
   "source": [
    "streetview_embeddings, flickr_embeddings = streetview_embeddings.to(device), flickr_embeddings.to(\n",
    "    device\n",
    ")\n",
    "sims = {}\n",
    "for s in tqdm(range(streetview_embeddings.shape[0])):\n",
    "    embed_sims = streetview_embeddings[s] @ flickr_embeddings.T\n",
    "    values, idxs = torch.topk(embed_sims, dim=-1, k=1)\n",
    "    s_id = s_ids[s]\n",
    "    r_id = r_ids[idxs.item()]\n",
    "    sims[s_id] = (values.item(), r_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each index, get the ground truth GPS\n",
    "latitudes, longitudes = [], []\n",
    "latitudes_gt, longitudes_gt = [], []\n",
    "for s_id, (prob, r_id) in sims.items():\n",
    "    s = streetview[s_id]\n",
    "    r = reference[r_id]\n",
    "\n",
    "    latitudes.append(r[\"LAT\"])\n",
    "    longitudes.append(r[\"LON\"])\n",
    "\n",
    "    latitudes_gt.append(s[\"LAT\"])\n",
    "    longitudes_gt.append(s[\"LON\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 0.0,\n",
       " 25: 0.01600000075995922,\n",
       " 200: 0.0949999988079071,\n",
       " 750: 0.3089999854564667,\n",
       " 2500: 0.6050000190734863}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latitudes, longitudes, latitudes_gt, longitudes_gt = (\n",
    "    torch.Tensor(latitudes),\n",
    "    torch.Tensor(longitudes),\n",
    "    torch.Tensor(latitudes_gt),\n",
    "    torch.Tensor(longitudes_gt),\n",
    ")\n",
    "distances = vectorized_gc_distance(latitudes, longitudes, latitudes_gt, longitudes_gt)\n",
    "gcd_threshold_eval(distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geoscreens",
   "language": "python",
   "name": "geoscreens"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
