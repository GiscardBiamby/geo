{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f70baa-482e-4f31-aa55-2c4c703da57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%autosave 60\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82aae395-bd63-4dad-b513-c8538651848e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import platform\n",
    "from copy import deepcopy\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Optional, Tuple, Union, cast\n",
    "\n",
    "import matplotlib as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import PIL.Image as pil_img\n",
    "from IPython.core.display import HTML, Markdown\n",
    "from IPython.display import Image, display\n",
    "from matplotlib_inline.backend_inline import set_matplotlib_formats\n",
    "from PIL import Image as pil_img\n",
    "from tqdm.contrib import tenumerate\n",
    "from tqdm.contrib.bells import tqdm\n",
    "\n",
    "from geoscreens.data import get_all_geoguessr_split_metadata\n",
    "from geoscreens.utils import load_json, save_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea3b28e-5ae7-4225-a1f7-c6e7df6da8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "pd.set_option(\"display.max_columns\", 15)\n",
    "pd.set_option(\"display.max_rows\", 50)\n",
    "# Suitable default display for floats\n",
    "pd.options.display.float_format = \"{:,.2f}\".format\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 10)\n",
    "\n",
    "# This one is optional -- change graphs to SVG only use if you don't have a\n",
    "# lot of points/lines in your graphs. Can also just use ['retina'] if you\n",
    "# don't want SVG.\n",
    "%config InlineBackend.figure_formats = [\"retina\"]\n",
    "set_matplotlib_formats(\"pdf\", \"png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a947e6af-f2e2-487e-8893-5cc88dd2584f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4b37e4-92f5-47fc-9073-643c488e8eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ingame = pickle.load(open(\"/shared/gbiamby/geo/segment/in_game_frames_000.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4782f87-823c-4f02-bb87-e89ab0457e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(\n",
    "    df_ingame.groupby([\"video_id\", \"img_width\", \"img_height\"]).agg(\n",
    "        total_frames=(\"sec\", \"count\"),\n",
    "        total_rounds=(\"round_num\", \"nunique\"),\n",
    "        # total_frames=(\"sec\", \"count\"),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af8f8cb1-e869-44f0-b042-7a5996480ee5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_ingame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1915f16-fb5d-4a15-ae84-fcf7e1cb7f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if True or \"df_meta\" not in locals():\n",
    "    df_meta = pd.DataFrame(\n",
    "        get_all_geoguessr_split_metadata(\n",
    "            force_include=[\"nemo_caption\", \"nemo_caption_entities\"]\n",
    "        ).values()\n",
    "    ).set_index(\"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516a27ad-b3ca-4965-a126-1af60472596b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_meta[\"video_id\"] = df_meta.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b348324-a8e4-4078-a224-b73825a5d3fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_meta.tail(2).T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144d11a7-ebfb-4bdd-ba1f-96c6cf66e2a4",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9b98ac-6441-4657-bbf7-6582ec2f9a0e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Match up the `ec` captions (the ones Grace generated clue similarities for) with Video Timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4f7591-b604-4adc-ba40-1d22d10d2df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize.punkt import PunktSentenceTokenizer\n",
    "\n",
    "\n",
    "def idx_to_keys(caption_mapping):\n",
    "    start, end = 0, 0\n",
    "    mapping = {}\n",
    "    for k in caption_mapping:\n",
    "        end += len(caption_mapping[k])\n",
    "        mapping[k] = (start, end)\n",
    "        start = end\n",
    "    return mapping\n",
    "\n",
    "\n",
    "def intersect(a, b):\n",
    "    return min(a[1], b[1]) - max(a[0], b[0]) > 0\n",
    "\n",
    "\n",
    "def sentence_to_timings(ann):\n",
    "    caption = \"\".join(ann[\"nemo_caption\"].values())\n",
    "    mapping = idx_to_keys(ann[\"nemo_caption\"])\n",
    "\n",
    "    idx = 0\n",
    "    keys = list(mapping.keys())\n",
    "    sentences = {}\n",
    "    for ent in ann[\"nemo_caption_entities\"]:\n",
    "        if ent[2] == \"sentence\":\n",
    "            timings = [k for k, v in mapping.items() if intersect(ent, v)]\n",
    "            subcaption = caption[ent[0] : ent[1]]\n",
    "            sentences[subcaption] = timings\n",
    "    return sentences\n",
    "\n",
    "\n",
    "def sentence_to_timings_punkt(ann):\n",
    "    caption = \"\".join(ann[\"nemo_caption\"].values()).strip()\n",
    "    mapping = idx_to_keys(ann[\"nemo_caption\"])\n",
    "\n",
    "    sentences = {}\n",
    "    tokenizer = PunktSentenceTokenizer()\n",
    "    subcaptions = list(tokenizer.tokenize(caption))\n",
    "    spans = list(tokenizer.span_tokenize(caption))\n",
    "    for span, subcaption in zip(spans, subcaptions):\n",
    "        timings = [k for k, v in mapping.items() if intersect(span, v)]\n",
    "        sentences[subcaption] = timings\n",
    "    return sentences\n",
    "\n",
    "\n",
    "def get_spans(caption: str, sentences: list[str]):\n",
    "    start = 0\n",
    "    end = len(sentences[0])\n",
    "    spans = []\n",
    "    for i, s in enumerate(sentences):\n",
    "        spans.append((start, end))\n",
    "        start = end\n",
    "        end += len(sentences[i + 1]) if i + 1 < len(sentences) else len(sentences)\n",
    "    return spans\n",
    "\n",
    "\n",
    "def sentence_to_timings_nltk(ann):\n",
    "    caption = \"\".join(ann[\"nemo_caption\"].values()).strip()\n",
    "    time_to_span = idx_to_keys(ann[\"nemo_caption\"])\n",
    "\n",
    "    sentences = OrderedDict()\n",
    "    subcaptions = list(nltk.tokenize.sent_tokenize(caption))\n",
    "    spans = get_spans(caption, subcaptions)\n",
    "    for i, (subcaption, span) in enumerate(zip(subcaptions, spans)):\n",
    "        timings = [\n",
    "            float(time) for time, _idx_span in time_to_span.items() if intersect(span, _idx_span)\n",
    "        ]\n",
    "        sentences[subcaption] = {\n",
    "            \"times\": timings,\n",
    "            \"idx\": i,\n",
    "            \"span\": span,\n",
    "            \"start\": min(timings),\n",
    "            \"end\": max(timings),\n",
    "        }\n",
    "    return sentences\n",
    "\n",
    "\n",
    "def get_meta():\n",
    "    \"\"\"Get metadata for all videos\"\"\"\n",
    "    df_meta = pd.DataFrame(\n",
    "        get_all_geoguessr_split_metadata(\n",
    "            force_include=[\"nemo_caption\", \"nemo_caption_entities\"]\n",
    "        ).values()\n",
    "    ).set_index(\"id\")\n",
    "    df_meta[\"video_id\"] = df_meta.index\n",
    "    return df_meta\n",
    "\n",
    "\n",
    "def load_clue_sims(dataset_type: str):\n",
    "    clue_sims = load_json(\n",
    "        f\"/shared/g-luo/geoguessr/data/data/guidebook/narrations/{dataset_type}.json\"\n",
    "    )\n",
    "    clue_sims = [\n",
    "        # {\"idx\": i, \"clue_sim\": clue_types[0], \"clue_cluster\": clue_types[1], **narration}\n",
    "        # for i, (narration, clue_types) in enumerate(\n",
    "        #     zip(clue_sims[\"narrations\"], clue_sims[\"clue_types\"])\n",
    "        # )\n",
    "        {\"idx\": i, **narration}\n",
    "        for i, (narration) in enumerate(clue_sims[\"narrations\"])\n",
    "    ]\n",
    "    # Update the index for each sentence so it starts at 0 for each video_id:\n",
    "    video_id = clue_sims[0][\"id\"]\n",
    "    idx = 0\n",
    "    for cs in clue_sims:\n",
    "        if cs[\"id\"] != video_id:\n",
    "            idx = 0\n",
    "            video_id = cs[\"id\"]\n",
    "        cs[\"idx\"] = idx\n",
    "        idx += 1\n",
    "\n",
    "    clue_sim_lookup = {(cs[\"id\"], cs[\"text\"], cs[\"idx\"]): cs for cs in clue_sims}\n",
    "    return clue_sims, clue_sim_lookup\n",
    "\n",
    "\n",
    "def get_caption_timings(df_meta: pd.DataFrame):\n",
    "    captions_nltk = {}\n",
    "    for i, video_id in tenumerate(df_meta.video_id.values, desc=\"get_caption_timings\"):\n",
    "        # if i[0] > 0:\n",
    "        #     break\n",
    "        captions_nltk[video_id] = sentence_to_timings_nltk(df_meta.loc[video_id].to_dict())\n",
    "    return captions_nltk\n",
    "\n",
    "\n",
    "def merge_timings_and_clue_sims(\n",
    "    captions_nltk, clue_sims: dict, clue_sim_lookup: list[tuple], clue_sim_ids: set[str]\n",
    "):\n",
    "    num_matches = 0\n",
    "    result = {}\n",
    "    for i, (video_id, sentences) in tenumerate(\n",
    "        captions_nltk.items(), desc=\"merge_timings_and_clue_sims\"\n",
    "    ):\n",
    "        if video_id not in clue_sim_ids:\n",
    "            continue\n",
    "        if video_id not in result:\n",
    "            result[video_id] = []\n",
    "        for sentence, sentence_info in sentences.items():\n",
    "            key = (video_id, sentence, sentence_info[\"idx\"])\n",
    "            if key in clue_sim_lookup:\n",
    "                num_matches += 1\n",
    "                result[video_id].append(\n",
    "                    {\n",
    "                        \"sentence\": sentence,\n",
    "                        \"clue_type\": clue_sim_lookup[key][\"clue_type\"],\n",
    "                        **deepcopy(sentence_info),\n",
    "                    }\n",
    "                )\n",
    "                # sentence_info[\"clue_sim\"] = clue_sim_lookup[key][\"clue_sim\"]\n",
    "                # sentence_info[\"clue_cluster\"] = clue_sim_lookup[key][\"clue_cluster\"]\n",
    "    print(\"num_matches: \", num_matches)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a93d0d-b3c8-4b6b-aa73-95bdd38757f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    if \"df_meta_original\" not in locals() or \"captions_nltk_original\" not in locals():\n",
    "        df_meta_original = get_meta()\n",
    "        captions_nltk_original = get_caption_timings(df_meta_original)\n",
    "    df_meta = deepcopy(df_meta_original)\n",
    "    captions_nltk = deepcopy(captions_nltk_original)\n",
    "    print(\"total videos: \", len(captions_nltk))\n",
    "\n",
    "    for dataset_type in [\"val\", \"test\", \"train\"]:\n",
    "        print(\"\\n\", \"=\" * 120, f\"\\n{dataset_type}\")\n",
    "        clue_sims, clue_sim_lookup = load_clue_sims(dataset_type)\n",
    "        print(f\"Total clue_sims: {len(clue_sims)}, clue_sims_lookup: {len(clue_sim_lookup)}\")\n",
    "        clue_sim_ids = {c[\"id\"] for c in clue_sims}\n",
    "        print(\n",
    "            f\"Total captions ({dataset_type}): \",\n",
    "            sum([len(t) for video_id, t in captions_nltk.items() if video_id in clue_sim_ids]),\n",
    "        )\n",
    "        result = merge_timings_and_clue_sims(\n",
    "            captions_nltk, clue_sims, clue_sim_lookup, clue_sim_ids\n",
    "        )\n",
    "        print(\n",
    "            f\"Merged sims + timings -- videos: {len(result)}, sentences: {sum([len(s) for s in result.values()])}\"\n",
    "        )\n",
    "        save_path = Path(f\"/shared/gbiamby/geo/captions/{dataset_type}_captions_with_timings.json\")\n",
    "        save_json(save_path, result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67c4266-86e1-4bd0-a87d-61768694d761",
   "metadata": {},
   "outputs": [],
   "source": [
    "split = \"val\"\n",
    "captions = load_json(f\"/shared/gbiamby/geo/captions/{split}_captions_with_timings.json\")\n",
    "\n",
    "\n",
    "def sort_captions(captions):\n",
    "    for video_id, caps in list(captions.items()):\n",
    "        captions[video_id] = sorted(caps, key=lambda x: x[\"idx\"])\n",
    "\n",
    "\n",
    "sort_captions(captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e570a4a0-a176-4d3e-ae6a-240ba194a364",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "list(captions.items())[0][1][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8021e701-4a8d-4604-a654-5f49113fd50a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "096e1465-58a7-4928-bb15-72fa507bf1d3",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c9ea05-d95c-44f6-b6ab-2027fec03237",
   "metadata": {},
   "source": [
    "## Simplest Approach: Map ASR Time to Image Time\n",
    "\n",
    "Each image should be paired with either 0 or one sentence. One sentence can be paired to many images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4721d20a-9e65-4948-8f1f-67d5719ec58b",
   "metadata": {},
   "outputs": [],
   "source": [
    "captions[\"-13sRRWmIxY\"][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05a3978-eebd-43f5-afd8-7eaea2b1759e",
   "metadata": {},
   "outputs": [],
   "source": [
    "clues_paragraphs = load_json(\n",
    "    \"/shared/g-luo/geoguessr/data/data/guidebook/text/clues/paragraphs.json\"\n",
    ")\n",
    "print(type(clues_paragraphs))\n",
    "print(len(clues_paragraphs))\n",
    "print(clues_paragraphs.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1f98c4-8b7c-4655-8ea7-10872bdd7225",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_captions(captions: dict[str, list[dict]]):\n",
    "    result = {}\n",
    "    for video_id, caps in captions.items():\n",
    "        if video_id not in result:\n",
    "            result[video_id] = []\n",
    "        for c in caps:\n",
    "            clue_sim, clue_cluster_id = c[\"clue_type\"][0], c[\"clue_type\"][1]\n",
    "            if (\"welcome back\" in c[\"sentence\"].casefold()) or (len(c[\"sentence\"]) < 4):\n",
    "                continue\n",
    "            if clue_cluster_id == 11:\n",
    "                if clue_sim >= 0.3:\n",
    "                    result[video_id].append(c)\n",
    "            elif clue_cluster_id == 4:\n",
    "                if clue_sim >= 0.2:\n",
    "                    result[video_id].append(c)\n",
    "            elif clue_sim >= 0.2:\n",
    "                result[video_id].append(c)\n",
    "        result[video_id] = sorted(result[video_id], key=lambda x: x[\"idx\"])\n",
    "    return result\n",
    "\n",
    "\n",
    "def ensure_no_time_overlaps(captions):\n",
    "    \"\"\"\n",
    "    Scan captions sequentially for each video and make sure the (start, end)\n",
    "    times for adjacent captions do not overlap. This check is good if we want to\n",
    "    pick images for each caption for each caption's time span.\n",
    "    \"\"\"\n",
    "    overlaps = []\n",
    "    for video_id, caps in captions.items():\n",
    "        for i in range(len(caps)):\n",
    "            cap = caps[i]\n",
    "            if i + 1 < len(caps) and cap[\"end\"] > caps[i + 1][\"start\"]:\n",
    "                overlaps.append((video_id, cap, caps[i + 1]))\n",
    "    print(\"Num overlaps: \", len(overlaps))\n",
    "    return overlaps\n",
    "\n",
    "\n",
    "def get_clip_samples_simple(\n",
    "    df_ingame: pd.DataFrame, captions: dict[str, list[dict[str, Any]]], method=\"simple\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Positive samples:\n",
    "        For each caption, maps images in the caption's (start, end) time range to the caption.\n",
    "    Negative samples:\n",
    "        For each caption, maps random images from the same video but different rounds to the caption.\n",
    "    \"\"\"\n",
    "    # fmt: off\n",
    "    frame_columns = [\n",
    "        \"round_num\", \"frame_idx\", \"img_width\", \"img_height\", \"sec\", \"time\",\n",
    "        \"labels\", \"scores\", \"bboxes\", \"split\", \"file_path\",\n",
    "    ]\n",
    "    # fmt: on\n",
    "    samples = []\n",
    "    no_frames = set()\n",
    "    for video_id, caps in tqdm(captions.items()):\n",
    "        video_has_pos_samples = False\n",
    "        if video_id not in df_ingame.index:\n",
    "            no_frames.add(video_id)\n",
    "            continue\n",
    "        df = df_ingame.loc[video_id]\n",
    "        for c in caps:\n",
    "            round_num = None\n",
    "            pos_samples = df[(c[\"start\"] <= df.sec) & (df.sec < c[\"end\"])]\n",
    "            if pos_samples is not None and len(pos_samples) > 0:\n",
    "                # positive sample(s):\n",
    "                round_num = pos_samples.iloc[0][\"round_num\"]\n",
    "                samples.append(\n",
    "                    {\n",
    "                        \"video_id\": video_id,\n",
    "                        \"caption_info\": c,\n",
    "                        \"frames\": pos_samples[frame_columns].to_dict(\"records\"),\n",
    "                        \"gt\": True,\n",
    "                    }\n",
    "                )\n",
    "                video_has_pos_samples = True\n",
    "                # negative sample(s):\n",
    "                neg_samples = df[~(df.round_num == round_num)].sample(len(pos_samples))\n",
    "                samples.append(\n",
    "                    {\n",
    "                        \"video_id\": video_id,\n",
    "                        \"caption_info\": c,\n",
    "                        \"frames\": neg_samples[frame_columns].to_dict(\"records\"),\n",
    "                        \"gt\": False,\n",
    "                    }\n",
    "                )\n",
    "        if not video_has_pos_samples:\n",
    "            no_frames.add(video_id)\n",
    "    return samples, no_frames\n",
    "\n",
    "\n",
    "def get_clip_samples(\n",
    "    df_ingame: pd.DataFrame, captions: dict[str, list[dict[str, Any]]], method=\"simple\"\n",
    "):\n",
    "    if method == \"simple\":\n",
    "        return get_clip_samples_simple(df_ingame, captions)\n",
    "    else:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "\n",
    "# overlaps = ensure_no_time_overlaps(captions)\n",
    "# captions_filtered = filter_captions(captions)\n",
    "# print(f\"Num videos: {len(captions_filtered):,}\")\n",
    "# print(f\"Num captions: {sum([len(caps) for caps in captions.values()]):,}\")\n",
    "# print(f\"Num filtered captions: {sum([len(caps) for caps in captions_filtered.values()]):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad74cc1-9ad0-4abc-bb78-6dce39fbc3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    pickle.dump(\n",
    "        captions_filtered,\n",
    "        open(f\"/shared/gbiamby/geo/captions/{split}_captions_with_timings_filtered.json\", \"wb\"),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf027dc4-c49e-4318-bd9a-5a9757f6cb38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# captions_filtered = pickle.load(open(f\"/shared/gbiamby/geo/captions/{split}_captions_with_timings_filtered.json\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d55c31be-f1f6-42b5-b83f-609f2dcad087",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pos, no_frames = get_clip_samples(df_ingame, captions_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc64608-6ea1-415c-91b6-bd4da3b3be9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Num captions w/ samples: {len(pos):,}\")\n",
    "print(f\"Num videos w/o any sampled frames: {len(no_frames):,}\")\n",
    "print(f\"Num videos w/ samples: {len({s['video_id'] for s in pos}):,}\")\n",
    "print(f\"Num samples: {sum([len(s['frames']) for s in pos]):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6826c9-8831-494b-a797-bdf1982dda1f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pos[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db639f26-6a5d-4c41-a445-757e316013e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_captions(captions):\n",
    "    for video_id, caps in list(captions.items()):\n",
    "        captions[video_id] = sorted(caps, key=lambda x: x[\"idx\"])\n",
    "\n",
    "\n",
    "def generate_samples(split):\n",
    "    df_ingame = pickle.load(open(\"/shared/gbiamby/geo/segment/in_game_frames_000.pkl\", \"rb\"))\n",
    "    captions = load_json(f\"/shared/gbiamby/geo/captions/{split}_captions_with_timings.json\")\n",
    "    sort_captions(captions)\n",
    "    overlaps = ensure_no_time_overlaps(captions)\n",
    "    captions_filtered = filter_captions(captions)\n",
    "    print(f\"Num videos: {len(captions_filtered):,}\")\n",
    "    print(f\"Num captions: {sum([len(caps) for caps in captions.values()]):,}\")\n",
    "    print(f\"Num filtered captions: {sum([len(caps) for caps in captions_filtered.values()]):,}\")\n",
    "    pickle.dump(\n",
    "        captions_filtered,\n",
    "        open(f\"/shared/gbiamby/geo/captions/{split}_captions_with_timings_filtered.json\", \"wb\"),\n",
    "    )\n",
    "\n",
    "    clip_samples, no_frames = get_clip_samples(df_ingame, captions_filtered)\n",
    "    print(f\"Num captions w/ samples: {len(clip_samples):,}\")\n",
    "    print(f\"Num videos w/o any sampled frames: {len(no_frames):,}\")\n",
    "    print(f\"Num videos w/ samples: {len({s['video_id'] for s in clip_samples}):,}\")\n",
    "    print(f\"Num samples: {sum([len(s['frames']) for s in clip_samples]):,}\")\n",
    "\n",
    "    save_json(f\"/shared/gbiamby/geo/captions/clip_samples_simple_{split}.json\", clip_samples)\n",
    "    print(\"Done!\")\n",
    "\n",
    "\n",
    "for split in [\"val\", \"test\", \"train\"]:\n",
    "    generate_samples(split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0762be-3bda-4f25-a576-d55023e84ee7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "idx = 1\n",
    "print(list(captions_filtered.items())[idx][0])\n",
    "list(captions_filtered.items())[idx][1][5:120]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0cea744-c7ab-4689-b797-63582e4aeb20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "52f407a8-52f7-4b24-8c50-22eb41a81c0c",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a824fbe-2f27-47c0-870b-39872ec9e4aa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3a0d72-d8e3-4e2a-98b1-63874f704f9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ad565d-0f42-40ae-823e-9dfc99bad535",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "37aa148d-23a5-4265-a75c-b44aa6d6fff2",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b10a9b-93e7-4508-bb54-7dea40ba0095",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a360ccc3-7909-477b-9368-cccea8b612a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c1560458-8b53-4428-a30a-4f3b52d9c30e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f3719e-f12d-42b3-8515-662a0f0552fc",
   "metadata": {},
   "source": [
    "## Debugging Why Sentence counts aren't matching up with the clue-sim counts\n",
    "\n",
    "Update: Solved. Issue was explicitly specifying the punkt tokenizer instead of using `nltk.tokenize` (which also uses punkt). The two methods give slightly different results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ac9b07-4527-481e-b762-5c5c8cd0caf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if \"captions_guide\" not in locals() or \"captions_guide_lookup\" not in locals():\n",
    "#     guide = load_json(f\"/shared/g-luo/geoguessr/data/data/guidebook/narrations/train.json\")\n",
    "#     captions_guide = {}\n",
    "#     captions_guide_lookup = {}\n",
    "#     for g in tqdm(guide[\"narrations\"]):\n",
    "#         if g[\"id\"] not in captions_guide:\n",
    "#             captions_guide[g[\"id\"]] = []\n",
    "#         captions_guide[g[\"id\"]].append(deepcopy(g))\n",
    "#         captions_guide_lookup[(g[\"id\"], g[\"text\"])] = g[\"clue_type\"]\n",
    "# # guide_lookup = {n[\"text\"]: n for n in guide[\"narrations\"]}\n",
    "# # type(guide[\"narrations\"]), guide[\"narrations\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27c4144-fce7-4770-87ec-311bc627aa56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"cg video_ids: \", len(captions_guide))\n",
    "# print(\"cg[K4GXuDACK40] sentences: \", len(captions_guide[\"K4GXuDACK40\"]))\n",
    "# print(\"cg total captions: \", len(captions_guide_lookup))\n",
    "# print(captions_guide[\"K4GXuDACK40\"][-10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6157fe8c-1e4a-4b5c-8cbb-96ad2680111c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print(len(captions_old[\"K4GXuDACK40\"]))\n",
    "# print(len(captions_new[\"K4GXuDACK40\"]))\n",
    "# print(len(captions_nltk[\"K4GXuDACK40\"]))\n",
    "\n",
    "# list(captions_old[\"K4GXuDACK40\"].items())[:10]\n",
    "# list(captions_nltk[\"K4GXuDACK40\"].items())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8fa260d-74ac-45f7-a85f-873ab4dbe90a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# if \"captions_old\" not in locals():\n",
    "#     captions_old = {}\n",
    "#     for t, video_id in tqdm(enumerate(df_meta.video_id.values)):\n",
    "#         captions_old[video_id] = sentence_to_timings(df_meta.loc[video_id].to_dict())\n",
    "\n",
    "# print(len(captions_old))\n",
    "# print(\n",
    "#     \"total captions \",\n",
    "#     sum([len(t) for video_id, t in captions_old.items() if video_id in captions_guide]),\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3212066-3346-4a44-80aa-8c95001e23f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# if \"captions_new\" not in locals():\n",
    "#     captions_new = {}\n",
    "#     for video_id in tqdm(df_meta.video_id.values):\n",
    "#         captions_new[video_id] = sentence_to_timings_punkt(df_meta.loc[video_id].to_dict())\n",
    "\n",
    "# print(len(captions_new))\n",
    "# sum([len(t) for video_id, t in captions_new.items() if video_id in captions_guide])\n",
    "# # list(captions_new.items())[-10:]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geoscreens",
   "language": "python",
   "name": "geoscreens"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
