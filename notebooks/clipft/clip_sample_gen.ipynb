{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f70baa-482e-4f31-aa55-2c4c703da57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%autosave 60\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82aae395-bd63-4dad-b513-c8538651848e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import platform\n",
    "from collections import OrderedDict\n",
    "from copy import deepcopy\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Optional, Tuple, Union, cast\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import PIL.Image as pil_img\n",
    "import seaborn as sns\n",
    "from IPython.core.display import HTML, Markdown\n",
    "from IPython.display import Image, display\n",
    "from matplotlib_inline.backend_inline import set_matplotlib_formats\n",
    "from nltk.tokenize.punkt import PunktSentenceTokenizer\n",
    "from PIL import Image as pil_img\n",
    "from tqdm.contrib import tenumerate\n",
    "from tqdm.contrib.bells import tqdm\n",
    "\n",
    "from geoscreens.data import get_all_geoguessr_split_metadata\n",
    "from geoscreens.utils import load_json, save_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea3b28e-5ae7-4225-a1f7-c6e7df6da8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "pd.set_option(\"display.max_columns\", 15)\n",
    "pd.set_option(\"display.max_rows\", 50)\n",
    "# Suitable default display for floats\n",
    "pd.options.display.float_format = \"{:,.2f}\".format\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 10)\n",
    "\n",
    "# This one is optional -- change graphs to SVG only use if you don't have a\n",
    "# lot of points/lines in your graphs. Can also just use ['retina'] if you\n",
    "# don't want SVG.\n",
    "%config InlineBackend.figure_formats = [\"retina\"]\n",
    "set_matplotlib_formats(\"pdf\", \"png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a947e6af-f2e2-487e-8893-5cc88dd2584f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4b37e4-92f5-47fc-9073-643c488e8eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ingame = pickle.load(open(\"/shared/gbiamby/geo/segment/in_game_frames_000.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4782f87-823c-4f02-bb87-e89ab0457e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(\n",
    "    df_ingame.groupby([\"video_id\", \"img_width\", \"img_height\"]).agg(\n",
    "        total_frames=(\"sec\", \"count\"),\n",
    "        total_rounds=(\"round_num\", \"nunique\"),\n",
    "        # total_frames=(\"sec\", \"count\"),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144d11a7-ebfb-4bdd-ba1f-96c6cf66e2a4",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9b98ac-6441-4657-bbf7-6582ec2f9a0e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "---\n",
    "\n",
    "## Match up the `ec` captions (the ones Grace generated clue similarities for) with Video Timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4f7591-b604-4adc-ba40-1d22d10d2df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def idx_to_keys(caption_mapping):\n",
    "    start, end = 0, 0\n",
    "    mapping = {}\n",
    "    for k in caption_mapping:\n",
    "        end += len(caption_mapping[k])\n",
    "        mapping[k] = (start, end)\n",
    "        start = end\n",
    "    return mapping\n",
    "\n",
    "\n",
    "def intersect(a, b):\n",
    "    return min(a[1], b[1]) - max(a[0], b[0]) > 0\n",
    "\n",
    "\n",
    "# def sentence_to_timings(ann):\n",
    "#     caption = \"\".join(ann[\"nemo_caption\"].values())\n",
    "#     mapping = idx_to_keys(ann[\"nemo_caption\"])\n",
    "\n",
    "#     idx = 0\n",
    "#     keys = list(mapping.keys())\n",
    "#     sentences = {}\n",
    "#     for ent in ann[\"nemo_caption_entities\"]:\n",
    "#         if ent[2] == \"sentence\":\n",
    "#             timings = [k for k, v in mapping.items() if intersect(ent, v)]\n",
    "#             subcaption = caption[ent[0] : ent[1]]\n",
    "#             sentences[subcaption] = timings\n",
    "#     return sentences\n",
    "\n",
    "\n",
    "# def sentence_to_timings_punkt(ann):\n",
    "#     caption = \"\".join(ann[\"nemo_caption\"].values()).strip()\n",
    "#     mapping = idx_to_keys(ann[\"nemo_caption\"])\n",
    "\n",
    "#     sentences = {}\n",
    "#     tokenizer = PunktSentenceTokenizer()\n",
    "#     subcaptions = list(tokenizer.tokenize(caption))\n",
    "#     spans = list(tokenizer.span_tokenize(caption))\n",
    "#     for span, subcaption in zip(spans, subcaptions):\n",
    "#         timings = [k for k, v in mapping.items() if intersect(span, v)]\n",
    "#         sentences[subcaption] = timings\n",
    "#     return sentences\n",
    "\n",
    "\n",
    "def get_spans(caption: str, sentences: list[str]):\n",
    "    start = 0\n",
    "    end = len(sentences[0])\n",
    "    spans = []\n",
    "    for i, s in enumerate(sentences):\n",
    "        spans.append((start, end))\n",
    "        start = end\n",
    "        end += len(sentences[i + 1]) if i + 1 < len(sentences) else len(sentences)\n",
    "    return spans\n",
    "\n",
    "\n",
    "def sentence_to_timings_nltk(ann):\n",
    "    caption = \"\".join(ann[\"nemo_caption\"].values()).strip()\n",
    "    time_to_span = idx_to_keys(ann[\"nemo_caption\"])\n",
    "\n",
    "    sentences = OrderedDict()\n",
    "    subcaptions = list(nltk.tokenize.sent_tokenize(caption))\n",
    "    spans = get_spans(caption, subcaptions)\n",
    "    for i, (subcaption, span) in enumerate(zip(subcaptions, spans)):\n",
    "        timings = [\n",
    "            float(time) for time, _idx_span in time_to_span.items() if intersect(span, _idx_span)\n",
    "        ]\n",
    "        sentences[subcaption] = {\n",
    "            \"times\": timings,\n",
    "            \"idx\": i,\n",
    "            \"span\": span,\n",
    "            \"start\": min(timings),\n",
    "            \"end\": max(timings),\n",
    "        }\n",
    "    return sentences\n",
    "\n",
    "\n",
    "def get_meta():\n",
    "    \"\"\"Get metadata for all videos\"\"\"\n",
    "    df_meta = pd.DataFrame(\n",
    "        get_all_geoguessr_split_metadata(\n",
    "            force_include=[\"nemo_caption\", \"nemo_caption_entities\"]\n",
    "        ).values()\n",
    "    ).set_index(\"id\")\n",
    "    df_meta[\"video_id\"] = df_meta.index\n",
    "    return df_meta\n",
    "\n",
    "\n",
    "def load_clue_sims(dataset_type: str):\n",
    "    clue_sims = load_json(\n",
    "        f\"/shared/g-luo/geoguessr/data/data/guidebook/narrations/{dataset_type}.json\"\n",
    "    )\n",
    "    clue_sims = [{\"idx\": i, **narration} for i, (narration) in enumerate(clue_sims[\"narrations\"])]\n",
    "    # Update the index for each sentence so it starts at 0 for each video_id:\n",
    "    video_id = clue_sims[0][\"id\"]\n",
    "    idx = 0\n",
    "    for cs in clue_sims:\n",
    "        if cs[\"id\"] != video_id:\n",
    "            idx = 0\n",
    "            video_id = cs[\"id\"]\n",
    "        cs[\"idx\"] = idx\n",
    "        idx += 1\n",
    "\n",
    "    clue_sim_lookup = {(cs[\"id\"], cs[\"text\"], cs[\"idx\"]): cs for cs in clue_sims}\n",
    "    return clue_sims, clue_sim_lookup\n",
    "\n",
    "\n",
    "def get_caption_timings(df_meta: pd.DataFrame):\n",
    "    captions_nltk = {}\n",
    "    for i, video_id in tenumerate(df_meta.video_id.values, desc=\"get_caption_timings\"):\n",
    "        # if i[0] > 0:\n",
    "        #     break\n",
    "        captions_nltk[video_id] = sentence_to_timings_nltk(df_meta.loc[video_id].to_dict())\n",
    "    return captions_nltk\n",
    "\n",
    "\n",
    "def merge_timings_and_clue_sims(\n",
    "    captions_nltk, clue_sims: dict, clue_sim_lookup: list[tuple], clue_sim_ids: set[str]\n",
    "):\n",
    "    num_matches = 0\n",
    "    result = {}\n",
    "    for i, (video_id, sentences) in tenumerate(\n",
    "        captions_nltk.items(), desc=\"merge_timings_and_clue_sims\"\n",
    "    ):\n",
    "        if video_id not in clue_sim_ids:\n",
    "            continue\n",
    "        if video_id not in result:\n",
    "            result[video_id] = []\n",
    "        for sentence, sentence_info in sentences.items():\n",
    "            key = (video_id, sentence, sentence_info[\"idx\"])\n",
    "            if key in clue_sim_lookup:\n",
    "                num_matches += 1\n",
    "                result[video_id].append(\n",
    "                    {\n",
    "                        \"sentence\": sentence,\n",
    "                        \"clue_type\": clue_sim_lookup[key][\"clue_type\"],\n",
    "                        **deepcopy(sentence_info),\n",
    "                    }\n",
    "                )\n",
    "    print(\"num_matches: \", num_matches)\n",
    "    return result\n",
    "\n",
    "\n",
    "def sort_captions(captions):\n",
    "    for video_id, caps in list(captions.items()):\n",
    "        captions[video_id] = sorted(caps, key=lambda x: x[\"idx\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd16a9c-e00a-4aef-9a7a-ee7e64acf4f5",
   "metadata": {},
   "source": [
    "#### Combine ASR/Clue Sims With ASR Timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a93d0d-b3c8-4b6b-aa73-95bdd38757f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    if \"df_meta_original\" not in locals() or \"captions_nltk_original\" not in locals():\n",
    "        df_meta_original = get_meta()\n",
    "        captions_nltk_original = get_caption_timings(df_meta_original)\n",
    "    df_meta = deepcopy(df_meta_original)\n",
    "    captions_nltk = deepcopy(captions_nltk_original)\n",
    "    print(\"total videos: \", len(captions_nltk))\n",
    "\n",
    "    for dataset_type in [\"val\", \"test\", \"train\"]:\n",
    "        print(\"\\n\", \"=\" * 120, f\"\\n{dataset_type}\")\n",
    "        clue_sims, clue_sim_lookup = load_clue_sims(dataset_type)\n",
    "        print(f\"Total clue_sims: {len(clue_sims)}, clue_sims_lookup: {len(clue_sim_lookup)}\")\n",
    "        clue_sim_ids = {c[\"id\"] for c in clue_sims}\n",
    "        print(\n",
    "            f\"Total captions ({dataset_type}): \",\n",
    "            sum([len(t) for video_id, t in captions_nltk.items() if video_id in clue_sim_ids]),\n",
    "        )\n",
    "        result = merge_timings_and_clue_sims(\n",
    "            captions_nltk, clue_sims, clue_sim_lookup, clue_sim_ids\n",
    "        )\n",
    "        print(\n",
    "            f\"Merged sims + timings -- videos: {len(result)}, sentences: {sum([len(s) for s in result.values()])}\"\n",
    "        )\n",
    "        save_path = Path(f\"/shared/gbiamby/geo/captions/{dataset_type}_captions_with_timings.json\")\n",
    "        # save_json(save_path, result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a606c7b9-abd2-462b-a983-4fe3086e71b6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Visual Inspection of ASR Sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961d3488-d341-4f37-9978-90916dc55182",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Inspect Sentence Time Windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67c4266-86e1-4bd0-a87d-61768694d761",
   "metadata": {},
   "outputs": [],
   "source": [
    "split = \"train\"\n",
    "captions = load_json(f\"/shared/gbiamby/geo/captions/{split}_captions_with_timings.json\")\n",
    "sort_captions(captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c98f50d-dfbe-4322-87f2-f7a3a75ef67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "clues_paragraphs = load_json(\n",
    "    \"/shared/g-luo/geoguessr/data/data/guidebook/text/clues/paragraphs.json\"\n",
    ")\n",
    "clue_clusters = list(clues_paragraphs.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476a307e-79f6-482d-a644-b33b41680294",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "flattened = []\n",
    "asr_sentences = [\n",
    "    [\n",
    "        {\n",
    "            \"video_id\": vid,\n",
    "            \"clue_cluster\": clue_clusters[c[\"clue_type\"][1]],\n",
    "            \"clue_sim\": c[\"clue_type\"][0],\n",
    "            \"dur\": c[\"end\"] - c[\"start\"],\n",
    "            **c,\n",
    "        }\n",
    "        for i, c in enumerate(caps)\n",
    "        if (c[\"clue_type\"][0] >= 0.4) and ((c[\"end\"] - c[\"start\"]) > 0)\n",
    "    ]\n",
    "    for vid, caps in captions.items()\n",
    "]\n",
    "# Re-index:\n",
    "for sentences in asr_sentences:\n",
    "    for i in range(len(sentences)):\n",
    "        sentences[i][\"idx\"] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81016ca-42ec-4424-9782-41a31fdddac6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "list(map(lambda x: flattened.extend(x), asr_sentences))\n",
    "len(flattened)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477e01d7-6444-4a5d-9780-d7f0a7ec8098",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_flat = pd.DataFrame(\n",
    "    flattened,\n",
    "    # columns=[\"idx\", \"video_id\", \"cluster_name\", \"clue_sim\", \"start\", \"end\", \"dur\", \"sentence\"],\n",
    ").sort_values([\"video_id\", \"idx\"])\n",
    "cuts, bins = pd.cut(df_flat.clue_sim, bins=10, retbins=True)\n",
    "df_flat[\"clue_bin\"] = cuts\n",
    "df_flat.set_index([\"video_id\", \"idx\"], drop=False, inplace=True, verify_integrity=True)\n",
    "df_flat.index.rename([\"_video_id\", \"_idx\"], inplace=True)\n",
    "display(df_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d307c1-17cc-499c-8e45-325afb965dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_flat[\"dur_bin\"], bins = pd.cut(df_flat.dur, bins=100, retbins=True)\n",
    "# df_flat.dur.plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104b38ed-6fb3-4bdc-abd1-575d5ed129fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_meta = get_meta()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af28f68-9aad-43ae-b4d3-748a220a85f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bins\n",
    "print(df_flat.shape)\n",
    "sns.histplot(df_flat[df_flat.dur <= 100].dur, bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dfe5e0b-2e79-4e05-9d4c-2f852dfd7253",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_flat.loc[\"zyZRvZohmro\"]\n",
    "# df_flat[df_flat.dur > 50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9f2be9-84c6-4ec3-ad9c-1265a4e0f690",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_meta.loc[\"-OYDoUERUqA\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe30e41-3fee-4df6-b31e-7ada36c834e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_tmp = df_flat.sample(50)\n",
    "print(df_tmp.shape)\n",
    "with pd.option_context(\"display.max_rows\", None, \"display.max_columns\", None):\n",
    "    # display(df_flat.loc[\"-OYDoUERUqA\"].head(100))\n",
    "    display(df_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e98345-1218-4a17-9a45-95267928f28c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_tmp = df_flat\n",
    "display(pd.DataFrame(df_tmp.groupby(\"clue_cluster\").agg(total=(\"idx\", \"count\"))).reset_index())\n",
    "print(f\"Total clues: {len(df_tmp):,}\")\n",
    "df_tmp.groupby(\"clue_cluster\").agg(total=(\"idx\", \"count\")).plot.bar(\n",
    "    title=\"Counts by Clue Cluster - clue_sim:>=0.4\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ecc2430-1ed3-4cca-aeb0-237ed3ec66b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_tmp = pd.DataFrame(df_flat.groupby(\"clue_bin\").agg(total=(\"idx\", \"count\"))).reset_index()\n",
    "display(df_tmp)\n",
    "df_tmp.plot.bar(x=\"clue_bin\", title=\"ASR Sentence/Clue Similarity Distribution (clue_sim>=0.4)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac01659-3a52-4a27-a68f-f40296000bc0",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Show Some ASR Sentence Examples (Random Sample Each Time the Cell is Run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213a23c0-01df-4ca5-88b2-dbae2695493c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "display(df_flat.sample(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bced435-fb72-4dc5-84d8-c81d120cc4ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "file = f\"/shared/g-luo/geoguessr/videos/bRSdHaz57Qk.en.vtt\"\n",
    "current = \"\"\n",
    "with open(file) as f:\n",
    "    text = f.readlines()\n",
    "\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8021e701-4a8d-4604-a654-5f49113fd50a",
   "metadata": {},
   "source": [
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "096e1465-58a7-4928-bb15-72fa507bf1d3",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c9ea05-d95c-44f6-b6ab-2027fec03237",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Generate Samples to Fine-tune CLIP - Simplest Approach: Map ASR Time Spans to Image Timestamps\n",
    "\n",
    "Each image should be paired with either 0 or one sentence. One sentence can be paired to many images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4721d20a-9e65-4948-8f1f-67d5719ec58b",
   "metadata": {},
   "outputs": [],
   "source": [
    "captions[\"-13sRRWmIxY\"][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05a3978-eebd-43f5-afd8-7eaea2b1759e",
   "metadata": {},
   "outputs": [],
   "source": [
    "clues_paragraphs = load_json(\n",
    "    \"/shared/g-luo/geoguessr/data/data/guidebook/text/clues/paragraphs.json\"\n",
    ")\n",
    "print(type(clues_paragraphs))\n",
    "print(len(clues_paragraphs))\n",
    "print(clues_paragraphs.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1f98c4-8b7c-4655-8ea7-10872bdd7225",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sort_captions(captions):\n",
    "    for video_id, caps in list(captions.items()):\n",
    "        captions[video_id] = sorted(caps, key=lambda x: x[\"idx\"])\n",
    "\n",
    "\n",
    "def filter_captions(captions: dict[str, list[dict]]):\n",
    "    result = {}\n",
    "    for video_id, caps in captions.items():\n",
    "        if video_id not in result:\n",
    "            result[video_id] = []\n",
    "        for c in caps:\n",
    "            clue_sim, clue_cluster_id = c[\"clue_type\"][0], c[\"clue_type\"][1]\n",
    "            if (\"welcome back\" in c[\"sentence\"].casefold()) or (len(c[\"sentence\"]) < 5):\n",
    "                continue\n",
    "            if (c[\"end\"] - c[\"start\"]) <= 0:\n",
    "                continue\n",
    "            elif clue_sim >= 0.4:\n",
    "                result[video_id].append(c)\n",
    "            # if clue_cluster_id == 11:\n",
    "            #     if clue_sim >= 0.3:\n",
    "            #         result[video_id].append(c)\n",
    "            # elif clue_cluster_id == 4:\n",
    "            #     if clue_sim >= 0.2:\n",
    "            #         result[video_id].append(c)\n",
    "            # elif clue_sim >= 0.2:\n",
    "            #     result[video_id].append(c)\n",
    "        result[video_id] = sorted(result[video_id], key=lambda x: x[\"idx\"])\n",
    "    return result\n",
    "\n",
    "\n",
    "def ensure_no_time_overlaps(captions):\n",
    "    \"\"\"\n",
    "    Scan captions sequentially for each video and make sure the (start, end)\n",
    "    times for adjacent captions do not overlap. This check is good if we want to\n",
    "    pick images for each caption for each caption's time span.\n",
    "    \"\"\"\n",
    "    overlaps = []\n",
    "    for video_id, caps in captions.items():\n",
    "        for i in range(len(caps)):\n",
    "            cap = caps[i]\n",
    "            if i + 1 < len(caps) and cap[\"end\"] > caps[i + 1][\"start\"]:\n",
    "                overlaps.append((video_id, cap, caps[i + 1]))\n",
    "    print(\"Num overlaps: \", len(overlaps))\n",
    "    return overlaps\n",
    "\n",
    "\n",
    "def to_fixed_time_windows(captions):\n",
    "    \"\"\"\n",
    "    Make each\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "def get_clip_samples_simple(df_ingame: pd.DataFrame, captions: dict[str, list[dict[str, Any]]]):\n",
    "    \"\"\"\n",
    "    Positive samples:\n",
    "        For each caption, maps images in the caption's (start, end) time range to the caption.\n",
    "    Negative samples:\n",
    "        For each caption, maps random images from the same video but different rounds to the caption.\n",
    "    \"\"\"\n",
    "    # fmt: off\n",
    "    frame_columns = [\n",
    "        \"round_num\", \"frame_idx\", \"img_width\", \"img_height\", \"sec\", \"time\",\n",
    "        \"labels\", \"scores\", \"bboxes\", \"split\", \"file_path\",\n",
    "    ]\n",
    "    # fmt: on\n",
    "    samples = []\n",
    "    no_frames = set()\n",
    "    for video_id, caps in tqdm(\n",
    "        captions.items(), desc=\"get_clip_samples_simple\", total=len(captions)\n",
    "    ):\n",
    "        video_has_pos_samples = False\n",
    "        if video_id not in df_ingame.index:\n",
    "            no_frames.add(video_id)\n",
    "            continue\n",
    "        df = df_ingame.loc[video_id]\n",
    "        for c in caps:\n",
    "            round_num = None\n",
    "            pos_samples = df[(c[\"start\"] <= df.sec) & (df.sec < c[\"end\"])]\n",
    "            if pos_samples is not None and len(pos_samples) > 0:\n",
    "                # positive sample(s):\n",
    "                round_num = pos_samples.iloc[0][\"round_num\"]\n",
    "                samples.append(\n",
    "                    {\n",
    "                        \"video_id\": video_id,\n",
    "                        \"caption_info\": c,\n",
    "                        \"frames\": pos_samples[frame_columns].to_dict(\"records\"),\n",
    "                        \"gt\": True,\n",
    "                    }\n",
    "                )\n",
    "                video_has_pos_samples = True\n",
    "                # negative sample(s):\n",
    "                neg_samples = df[~(df.round_num == round_num)].sample(len(pos_samples))\n",
    "                samples.append(\n",
    "                    {\n",
    "                        \"video_id\": video_id,\n",
    "                        \"caption_info\": c,\n",
    "                        \"frames\": neg_samples[frame_columns].to_dict(\"records\"),\n",
    "                        \"gt\": False,\n",
    "                    }\n",
    "                )\n",
    "        if not video_has_pos_samples:\n",
    "            no_frames.add(video_id)\n",
    "    return samples, no_frames\n",
    "\n",
    "\n",
    "def get_clip_samples_fixed_window(\n",
    "    df_ingame: pd.DataFrame, captions: dict[str, list[dict[str, Any]]], time_window: float = 5.0\n",
    "):\n",
    "    \"\"\"\n",
    "    Positive samples:\n",
    "        For each caption, maps images in the caption's (start, end) time range to the caption.\n",
    "    Negative samples:\n",
    "        For each caption, maps random images from the same video but different rounds to the caption.\n",
    "    \"\"\"\n",
    "    # fmt: off\n",
    "    frame_columns = [\n",
    "        \"round_num\", \"frame_idx\", \"img_width\", \"img_height\", \"sec\", \"time\",\n",
    "        \"labels\", \"scores\", \"bboxes\", \"split\", \"file_path\",\n",
    "    ]\n",
    "    # fmt: on\n",
    "    samples = []\n",
    "    no_frames = set()\n",
    "    for video_id, caps in tqdm(\n",
    "        captions.items(), desc=\"get_clip_samples_fixed_window\", total=len(captions)\n",
    "    ):\n",
    "        video_has_pos_samples = False\n",
    "        if video_id not in df_ingame.index:\n",
    "            no_frames.add(video_id)\n",
    "            continue\n",
    "        df = df_ingame.loc[video_id]\n",
    "        for c in caps:\n",
    "            round_num = None\n",
    "            anchor = (c[\"start\"], c[\"end\"])\n",
    "            pos_samples = df[(anchor[0] - time_window <= df.sec) * (anchor[0] + 1 > df.sec)]\n",
    "            if pos_samples is not None and len(pos_samples) > 0:\n",
    "                # positive sample(s):\n",
    "                round_num = pos_samples.iloc[0][\"round_num\"]\n",
    "                samples.append(\n",
    "                    {\n",
    "                        \"video_id\": video_id,\n",
    "                        \"caption_info\": c,\n",
    "                        \"frames\": pos_samples[frame_columns].to_dict(\"records\"),\n",
    "                        \"anchor\": anchor,\n",
    "                        \"gt\": True,\n",
    "                    }\n",
    "                )\n",
    "                video_has_pos_samples = True\n",
    "        if not video_has_pos_samples:\n",
    "            no_frames.add(video_id)\n",
    "    return samples, no_frames\n",
    "\n",
    "\n",
    "def get_clip_samples(\n",
    "    df_ingame: pd.DataFrame, captions: dict[str, list[dict[str, Any]]], method=\"simple\"\n",
    "):\n",
    "    if method == \"simple\":\n",
    "        return get_clip_samples_simple(df_ingame, captions)\n",
    "    elif method == \"fixed_window\":\n",
    "        return get_clip_samples_fixed_window(df_ingame, captions)\n",
    "    else:\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd9e2c4-4640-4b40-9df7-2266dae6e014",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Old Code to Create Captions for a Single Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad74cc1-9ad0-4abc-bb78-6dce39fbc3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if False:\n",
    "#     overlaps = ensure_no_time_overlaps(captions_nltk)\n",
    "#     captions_filtered = filter_captions(captions_nltk)\n",
    "#     print(f\"Num videos: {len(captions_filtered):,}\")\n",
    "#     print(f\"Num captions: {sum([len(caps) for caps in captions.values()]):,}\")\n",
    "#     print(f\"Num filtered captions: {sum([len(caps) for caps in captions_filtered.values()]):,}\")\n",
    "#     pickle.dump(\n",
    "#         captions_filtered,\n",
    "#         open(f\"/shared/gbiamby/geo/captions/{split}_captions_with_timings_filtered.json\", \"wb\"),\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf027dc4-c49e-4318-bd9a-5a9757f6cb38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# captions_filtered = pickle.load(open(f\"/shared/gbiamby/geo/captions/{split}_captions_with_timings_filtered.json\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d55c31be-f1f6-42b5-b83f-609f2dcad087",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pos, no_frames = get_clip_samples(df_ingame, captions_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc64608-6ea1-415c-91b6-bd4da3b3be9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"Num captions w/ samples: {len(pos):,}\")\n",
    "# print(f\"Num videos w/o any sampled frames: {len(no_frames):,}\")\n",
    "# print(f\"Num videos w/ samples: {len({s['video_id'] for s in pos}):,}\")\n",
    "# print(f\"Num samples: {sum([len(s['frames']) for s in pos]):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6826c9-8831-494b-a797-bdf1982dda1f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pos[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b78f8b-1507-4161-b496-d58502b2cc4c",
   "metadata": {},
   "source": [
    "#### Generate CLIP Samples for All Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db639f26-6a5d-4c41-a445-757e316013e0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def flatten_samples(samples: list[dict[str, Any]]):\n",
    "    flattened = []\n",
    "    drop = [\"bboxes\", \"scores\", \"labels\"]\n",
    "    # list(map(lambda x: flattened.extend(x), samples.values()))\n",
    "    for s in samples:\n",
    "        caption_info = s[\"caption_info\"]\n",
    "        base = {\n",
    "            # \"video_id\": s[\"video_id\"],\n",
    "            # \"gt\": s[\"gt\"],\n",
    "            **{k: v for k, v in s.items() if k not in (\"caption_info\", \"frames\")},\n",
    "            **caption_info,\n",
    "        }\n",
    "        for f in s[\"frames\"]:\n",
    "            s_flat = {**base, **f}\n",
    "            s_flat = deepcopy(s_flat)\n",
    "            for k in drop:\n",
    "                if k in s_flat:\n",
    "                    del s_flat[k]\n",
    "            flattened.append(s_flat)\n",
    "    print(f\"len(samples): {len(samples):,}\")\n",
    "    print(f\"len(flattened): {len(flattened):,}\")\n",
    "    return flattened\n",
    "\n",
    "\n",
    "def generate_samples(split):\n",
    "    print(\"\\n\", \"=\" * 120)\n",
    "    df_ingame = pickle.load(open(\"/shared/gbiamby/geo/segment/in_game_frames_000.pkl\", \"rb\"))\n",
    "    # captions = load_json(f\"/shared/gbiamby/geo/captions/{split}_captions_with_timings.json\")\n",
    "    # sort_captions(captions)\n",
    "    # overlaps = ensure_no_time_overlaps(captions)\n",
    "    # captions_filtered = filter_captions(captions)\n",
    "    # print(f\"Num videos: {len(captions_filtered):,}\")\n",
    "    # print(f\"Num captions: {sum([len(caps) for caps in captions.values()]):,}\")\n",
    "    # print(f\"Num filtered captions: {sum([len(caps) for caps in captions_filtered.values()]):,}\")\n",
    "    # pickle.dump(\n",
    "    #     captions_filtered,\n",
    "    #     open(f\"/shared/gbiamby/geo/captions/{split}_captions_with_timings_filtered.json\", \"wb\"),\n",
    "    # )\n",
    "    captions_filtered = pickle.load(\n",
    "        open(f\"/shared/gbiamby/geo/captions/{split}_captions_with_timings_filtered.json\", \"rb\")\n",
    "    )\n",
    "\n",
    "    method = \"fixed_window\"\n",
    "    clip_samples, no_frames = get_clip_samples(df_ingame, captions_filtered, method=method)\n",
    "    print(f\"Num captions w/ samples: {len(clip_samples):,}\")\n",
    "    print(f\"Num videos w/o any sampled frames: {len(no_frames):,}\")\n",
    "    print(f\"Num videos w/ samples: {len({s['video_id'] for s in clip_samples}):,}\")\n",
    "    print(f\"Num samples: {sum([len(s['frames']) for s in clip_samples]):,}\")\n",
    "    flattened = flatten_samples(clip_samples)\n",
    "    save_json(f\"/shared/gbiamby/geo/captions/clip_samples_{method}_{split}.json\", flattened)\n",
    "    df = pd.DataFrame(flattened)\n",
    "    print(f\"Total frames: {len(df)}, unique frames: {df.file_path.nunique()}\")\n",
    "    df.to_csv(f\"/shared/gbiamby/geo/captions/clip_samples_{method}_{split}_full.csv\", index=False)\n",
    "    df = df[df[\"gt\"]][[\"file_path\", \"sentence\"]]\n",
    "    df.to_csv(\n",
    "        f\"/shared/gbiamby/geo/captions/clip_samples_{method}_{split}_openclip.csv\", index=False\n",
    "    )\n",
    "    print(\"Done!\")\n",
    "    return df\n",
    "\n",
    "\n",
    "for split in [\"val\", \"test\", \"train\"]:\n",
    "    # for split in [\"val\"]:\n",
    "    df = generate_samples(split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0762be-3bda-4f25-a576-d55023e84ee7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "idx = 1\n",
    "print(list(captions_filtered.items())[idx][0])\n",
    "list(captions_filtered.items())[idx][1][5:120]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0cea744-c7ab-4689-b797-63582e4aeb20",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for split in [\"val\", \"test\", \"train\"]:\n",
    "    clip_samples = load_json(f\"/shared/gbiamby/geo/captions/clip_samples_simple_{split}.json\")\n",
    "    print(\"\")\n",
    "    print(\"=\" * 100)\n",
    "    print(f\"Num captions w/ samples: {len(clip_samples):,}\")\n",
    "    print(f\"Num videos w/ samples: {len({s['video_id'] for s in clip_samples}):,}\")\n",
    "    print(f\"Num samples: {sum([len(s['frames']) for s in clip_samples]):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e215a2c3-1dbf-41f7-be2b-547cd3af84a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a55203-cc2c-42de-9d00-8346a56f5682",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clip = pd.read_csv(f\"/shared/gbiamby/geo/captions/clip_samples_fixed_window_train_full.csv\")\n",
    "print(\"total samples: \", len(df_clip), \"unique frames: \", df_clip.file_path.nunique())\n",
    "df_clip[\"file_count\"] = df_clip.join(\n",
    "    pd.DataFrame(df_clip.groupby(\"file_path\").agg(file_count=(\"file_path\", \"count\"))),\n",
    "    on=\"file_path\",\n",
    ")[[\"file_count\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958adb0e-2598-486e-a9a2-7abc49215bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.option_context(\"display.max_rows\", None, \"display.max_columns\", None):\n",
    "    display(df_clip.head(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a01284d-813c-4680-be64-812b0bd3219b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "52f407a8-52f7-4b24-8c50-22eb41a81c0c",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b41381c0-788a-49ff-bbbe-735dee097631",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Visualize Some of the CLIP +/- Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fccc141-b1b3-4685-ba4d-97d929398552",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_grid(images: np.ndarray, max_rows=4, max_cols=2):\n",
    "    fig, axes = plt.subplots(nrows=max_rows, ncols=max_cols, figsize=(40, 40))\n",
    "    for idx, image in enumerate(images[: max_rows * max_cols]):\n",
    "        row = idx // max_cols\n",
    "        col = idx % max_cols\n",
    "        axes[row, col].axis(\"off\")\n",
    "        axes[row, col].imshow(image, cmap=\"gray\", aspect=\"auto\")\n",
    "    plt.subplots_adjust(wspace=0.05, hspace=0.05)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def show_asr_sentence_and_video_frame_samples(video_info):\n",
    "    # video_info = list(clip_samples)[0]\n",
    "    video_id = video_info[\"video_id\"]\n",
    "    print(\"video_id: \", video_id)\n",
    "    print(\"Sentence: \", video_info[\"caption_info\"][\"sentence\"])\n",
    "    print(video_info[\"caption_info\"][\"clue_type\"])\n",
    "    print(\n",
    "        \"Clue Cluster: \", list(clues_paragraphs.keys())[video_info[\"caption_info\"][\"clue_type\"][1]]\n",
    "    )\n",
    "    print(\"Num frames: \", len(video_info[\"frames\"]))\n",
    "    print(\"Sample types: \", \"Positive\" if video_info[\"gt\"] else \"Negative\")\n",
    "    # print([(frame[\"file_path\"], Path(frame[\"file_path\"]).exists()) for frame in video_info[\"frames\"]])\n",
    "    # for f in video_info[\"frames\"]:\n",
    "    #     print(f)\n",
    "    imgs = [pil_img.open(frame[\"file_path\"]) for frame in video_info[\"frames\"]]\n",
    "    plot_grid(imgs)\n",
    "    video_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a824fbe-2f27-47c0-870b-39872ec9e4aa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "clues_paragraphs = load_json(\n",
    "    \"/shared/g-luo/geoguessr/data/data/guidebook/text/clues/paragraphs.json\"\n",
    ")\n",
    "# show_video_info(np.random.choice(clip_samples, 1)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372f0edd-8e3e-43a0-836d-93cafe2c1dd7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "show_asr_sentence_and_video_frame_samples(\n",
    "    [cs for cs in clip_samples if cs[\"video_id\"] == \"HEPyfvK-Vhg\"][0]\n",
    ")\n",
    "# np.random.choice([cs for cs in clip_samples if cs[\"video_id\"]==\"HEPyfvK-Vhg\"], 1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3a0d72-d8e3-4e2a-98b1-63874f704f9b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "show_asr_sentence_and_video_frame_samples(np.random.choice(clip_samples, 1)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ad565d-0f42-40ae-823e-9dfc99bad535",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "show_asr_sentence_and_video_frame_samples(np.random.choice(clip_samples, 1)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d558396a-367c-45da-943e-516d5d7387ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "show_asr_sentence_and_video_frame_samples(np.random.choice(clip_samples, 1)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aefaa3f9-2e84-4d07-be34-f12e0ec94ddd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "show_asr_sentence_and_video_frame_samples(np.random.choice(clip_samples, 1)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9cd723-7dd7-497b-8857-458dc26b4cf3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0ce51f-2426-46db-9830-69c6f6ac0542",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b40defa-e45f-4ae6-aac8-77e00fd72172",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "37aa148d-23a5-4265-a75c-b44aa6d6fff2",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b10a9b-93e7-4508-bb54-7dea40ba0095",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a360ccc3-7909-477b-9368-cccea8b612a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c1560458-8b53-4428-a30a-4f3b52d9c30e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f3719e-f12d-42b3-8515-662a0f0552fc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Debugging Why Sentence counts aren't matching up with the clue-sim counts\n",
    "\n",
    "Update: **__Solved__**. Issue was explicitly specifying the punkt tokenizer instead of using `nltk.tokenize` (which also uses punkt). The two methods give slightly different results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ac9b07-4527-481e-b762-5c5c8cd0caf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if \"captions_guide\" not in locals() or \"captions_guide_lookup\" not in locals():\n",
    "#     guide = load_json(f\"/shared/g-luo/geoguessr/data/data/guidebook/narrations/train.json\")\n",
    "#     captions_guide = {}\n",
    "#     captions_guide_lookup = {}\n",
    "#     for g in tqdm(guide[\"narrations\"]):\n",
    "#         if g[\"id\"] not in captions_guide:\n",
    "#             captions_guide[g[\"id\"]] = []\n",
    "#         captions_guide[g[\"id\"]].append(deepcopy(g))\n",
    "#         captions_guide_lookup[(g[\"id\"], g[\"text\"])] = g[\"clue_type\"]\n",
    "# # guide_lookup = {n[\"text\"]: n for n in guide[\"narrations\"]}\n",
    "# # type(guide[\"narrations\"]), guide[\"narrations\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27c4144-fce7-4770-87ec-311bc627aa56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"cg video_ids: \", len(captions_guide))\n",
    "# print(\"cg[K4GXuDACK40] sentences: \", len(captions_guide[\"K4GXuDACK40\"]))\n",
    "# print(\"cg total captions: \", len(captions_guide_lookup))\n",
    "# print(captions_guide[\"K4GXuDACK40\"][-10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6157fe8c-1e4a-4b5c-8cbb-96ad2680111c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print(len(captions_old[\"K4GXuDACK40\"]))\n",
    "# print(len(captions_new[\"K4GXuDACK40\"]))\n",
    "# print(len(captions_nltk[\"K4GXuDACK40\"]))\n",
    "\n",
    "# list(captions_old[\"K4GXuDACK40\"].items())[:10]\n",
    "# list(captions_nltk[\"K4GXuDACK40\"].items())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8fa260d-74ac-45f7-a85f-873ab4dbe90a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# if \"captions_old\" not in locals():\n",
    "#     captions_old = {}\n",
    "#     for t, video_id in tqdm(enumerate(df_meta.video_id.values)):\n",
    "#         captions_old[video_id] = sentence_to_timings(df_meta.loc[video_id].to_dict())\n",
    "\n",
    "# print(len(captions_old))\n",
    "# print(\n",
    "#     \"total captions \",\n",
    "#     sum([len(t) for video_id, t in captions_old.items() if video_id in captions_guide]),\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3212066-3346-4a44-80aa-8c95001e23f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# if \"captions_new\" not in locals():\n",
    "#     captions_new = {}\n",
    "#     for video_id in tqdm(df_meta.video_id.values):\n",
    "#         captions_new[video_id] = sentence_to_timings_punkt(df_meta.loc[video_id].to_dict())\n",
    "\n",
    "# print(len(captions_new))\n",
    "# sum([len(t) for video_id, t in captions_new.items() if video_id in captions_guide])\n",
    "# # list(captions_new.items())[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a6b0d0c-7a83-4c50-9b8b-74fddc21eac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from geoscreens.consts import FRAMES_METADATA_PATH\n",
    "# from geoscreens.utils import load_json\n",
    "# fm = load_json(FRAMES_METADATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5bac82-78bd-4532-8dc5-a4f565bc477d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geoscreens",
   "language": "python",
   "name": "geoscreens"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
