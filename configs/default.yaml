project_name: "geoscreens"
exp_id: null
seed: 42
gpus: [1]

env:
  # Similar to cache dir, but can be used if specifically want to override where MMF stores your
  # data. Default would be cache_dir/data. We will auto download models and datasets in this folder
  data_dir: ${resolve_dir:ICEDET_DATA_DIR, datasets}

  # Directory for saving checkpoints and other metadata Use ICEDET_SAVE_DIR or env.save_dir to
  # override. This will get the exp_id appended as a subdirectory automatically.
  save_dir: ${env:ICEDET_SAVE_DIR, ./output}

  # Log directory for Weights and Biases, default points to same as logs Only used when
  # training.wandb is enabled. Use ICEDET_WANDB_LOGDIR or env.wandb_logdir to override
  wandb_logdir: ${env:ICEDET_WANDB_LOGDIR,}

model_config:
  framework: "torchvision"
  name: "retinanet"
  # backbone: "resnet_50_fpn"

dataset_config:
  dataset_name: geoscreens_001
  # If a relative path, it's relative to PROJECT_ROOT (aka root of the git repo if you've cloned
  # this project);
  data_root: "./datasets"
  # If a relative path, it's relative to PROJECT_ROOT (aka root of the git repo if you've cloned
  # this project);
  img_dir: "./datasets/images"
  img_size: 640
  batch_size: ${training.batch_size}
  num_workers: ${training.num_workers}
  pin_memory: ${training.pin_memory}

training:
  # Name of the experiment, will be used while saving checkpoints and generating reports
  experiment_name: run
  # Size of the batch globally. If distributed or data_parallel is used, this will be divided
  # equally among GPUs
  batch_size: 12
  # Number of workers to be used in dataloaders
  num_workers: 4
  # Whether to pin memory in dataloader
  pin_memory: false
  learning_rate: 1e-4

  # After `checkpoint_interval` number of updates, MMF will make a snapshot which will involve
  # creating a checkpoint for current training scenarios
  checkpoint_interval: 1000
  # This will evaluate evaluation metrics on whole validation set after evaluation interval number
  # of updates
  evaluation_interval: 1000

  params:
    max_epochs: 1
    gpus: [1]
    precision: 16
    amp_backend: "native"
    check_val_every_n_epoch: 5

  early_stop:
    # Whether to use early stopping, (Default: false)
    enabled: false
    # Patience for early stoppings
    patience: 4000
    # Criteria to be monitored for early stopping total_loss will monitor combined loss from all of
    # the tasks Criteria can also be an evaluation metric in this format `dataset/metric` for e.g.
    # vqa2/vqa_accuracy
    criteria: total_loss
    # Whether the monitored criteria should be minimized for early stopping or not, for e.g. you
    # would want to minimize loss but maximize an evaluation metric like accuracy etc.
    minimize: true

  # Weights and Biases control, by default Weights and Biases (wandb) is disabled
  wandb:
    # Whether to use Weights and Biases Logger, (Default: false)
    enabled: True
    # Project name to be used while logging the experiment with wandb
    project: geoscreens_${oc.env:USER}
    # Experiment/ run name to be used while logging the experiment under the project with wandb
    name: ${training.experiment_name}
    #  Log checkpoints created by
    #    :class:`~pytorch_lightning.callbacks.model_checkpoint.ModelCheckpoint` as W&B artifacts.
    #    `latest` and `best` aliases are automatically set.
    #
    #      * if ``log_model == 'all'``, checkpoints are logged during training.
    #      * if ``log_model == True``, checkpoints are logged at the end of training, except when
    #        :paramref:`~pytorch_lightning.callbacks.model_checkpoint.ModelCheckpoint.save_top_k` ``== -1``
    #        which also logs every checkpoint during training.
    #      * if ``log_model == False`` (default), no checkpoint is logged.
    log_model: True
    # Path where data is saved (wandb dir by default).
    save_dir: ${env.wandb_logdir}
    offline: False
    # Sets the version, mainly used to resume a previous run.
    id: null
    # Enables or explicitly disables anonymous logging.
    anonymous: null
    # A string to put at the beginning of metric keys.
    prefix: ""
