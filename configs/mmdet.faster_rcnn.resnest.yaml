seed: 42
gpus: 2

model_config:
  # Options: torchvision, mmdet, ross, ultralytics
  framework: "mmdet"
  name: "faster_rcnn"
  backbone: "resnest50_fpn"
  # params:
  #   detections_per_img: 512
  #   # Anchor box sizes. Default:  [32, 64, 128, 256, 512]
  #   anchor_sizes: [32, 64, 128, 256, 512]
  #   # Anchor box aspect ratios (h/w). Default: [0.5, 1.0, 2.0]
  #   aspect_ratios: [0.08, 0.16, 0.25, 0.36, 0.5, 0.7, 1.0, 2.0]
  #   score_thresh: 0.05
  #   nms_thresh: 0.5
  #   fg_iou_thresh: 0.5
  #   bg_iou_thresh: 0.4

training:
  # Name of the experiment, will be used while saving checkpoints and generating reports
  experiment_name: run
  # Size of the batch globally. If distributed or data_parallel is used, this will be divided
  # equally among GPUs
  batch_size: 8
  # Number of workers to be used in dataloaders
  num_workers: 8
  # Whether to pin memory in dataloader
  pin_memory: false
  # After `checkpoint_interval` number of updates, MMF will make a snapshot which will involve
  # creating a checkpoint for current training scenarios
  checkpoint_interval: 1000
  # This will evaluate evaluation metrics on whole validation set after evaluation interval number
  # of updates
  evaluation_interval: 1000

  # These are passed to the constructor for the Lightning Trainer
  params:
    max_epochs: 50
    gpus: 2
    precision: 32
    amp_backend: "native"
    check_val_every_n_epoch: 5

  early_stop:
    # Whether to use early stopping, (Default: false)
    enabled: true
    params:
      # Patience for early stoppings
      patience: 4
      # Criteria to be monitored for early stopping total_loss will monitor combined loss from all of
      # the tasks Criteria can also be an evaluation metric in this format `dataset/metric` for e.g.
      # vqa2/vqa_accuracy
      monitor: "train/loss"
      # Whether the monitored criteria should be minimized for early stopping or not, for e.g. you
      # would want to minimize loss but maximize an evaluation metric like accuracy etc.
      mode: "min"
      min_delta: 0.0

optimizer:
  type: "adam"
  params:
    lr: 1e-3
    # betas: null
    # eps: 1e-8
    # weight_decay: 0
    # amsgrad: false

scheduler:
  type: "ReduceLROnPlateau"
  params:
    mode: "min"
    factor: 0.6
    patience: 10
    min_lr: 1e-7
    verbose: True
  config:
    # The unit of the scheduler's step size, could also be 'step'. 'epoch' updates the scheduler on
    # epoch end whereas 'step' updates it after a optimizer update.
    interval: "epoch"
    # Metric to to monitor for schedulers like `ReduceLROnPlateau`
    monitor: "train/loss"
    # If set to `True`, will enforce that the value specified 'monitor' is available when the
    # scheduler is updated, thus stopping training if not found. If set to `False`, it will only
    # produce a warning
    strict: True
    # How many epochs/steps should pass between calls to `scheduler.step()`. 1 corresponds to
    # updating the learning rate after every epoch/step.
    #
    # If "monitor" references validation metrics, then "frequency" should be set to a multiple of
    # "trainer.check_val_every_n_epoch".
    frequency: 1
    # If using the `LearningRateMonitor` callback to monitor the learning rate progress, this
    # keyword can be used to specify a custom logged name
    name: None
